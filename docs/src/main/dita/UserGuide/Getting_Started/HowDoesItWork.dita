<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_bl5_dl4_1r">
 <title>How does this really work?</title>
 <shortdesc>Let's walk through it...</shortdesc>
 <conbody>
  <p>After you install and launch <ph
    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>, you access
   the <ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
   console, log in, and create your first pipeline. </p>
  <p>What do you want it to do? Let's say you want to read XML files from a directory and remove the
   newline characters before moving it into HDFS. To do this, you start with a Directory origin
   stage and configure it to point to the source file directory. (You can also have the stage
   archive processed files and write files that were not fully processed to a separate directory for
   review.)</p>
  <p>To remove the newline characters, connect Directory to an Expression Evaluator processor and
   configure it to remove the newline character from the last field in the record.</p>
  <p>To make the data available to HDFS, you connect the Expression Evaluator to a Hadoop FS
   destination stage. You configure the stage to write the data as a JSON object (though you can use
   other data formats as well). </p>
  <p>You preview data to see how source data moves through the pipeline and notice that some fields
   have missing data. So you add a Value Replacer to replace null values in those fields. </p>
  <p>Now that the data flow is done, you configure the pipeline error record handling to write error
   records to a file, you create a data drift alert to let you know when field names change, and you
   configure an email alert to let you know when the pipeline generates more than 100 error records.
   Then, you start the pipeline and <ph
    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> goes to work. </p>
  <p>The <ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
   console goes into Monitor mode and displays summary and error statistics immediately. To get a
   closer look at the activity, you take a snapshot of the pipeline so you can examine how a set of
   data passed through the pipeline. You see some unexpected data in the pipeline, so you create a
   data rule for a link between two stages to gather information about similar data and set an alert
   to notify you when the numbers get too high.</p>
  <p>And what about those error records being written to file? They're saved with error details, so
   you can create an error pipeline to reprocess that data. Et voila!</p>
  <p>StreamSets <ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
   /> is a powerful tool, but we're making it as simple as possible to use. So give it a try, click
   the Help icon for information, and contact us if you need a hand. </p>
 </conbody>
</concept>
