<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_hpr_twm_jq">
 <title>Origins</title>
 <shortdesc>An origin stage represents the source for the pipeline. You can use a single origin
    stage in a pipeline.</shortdesc>
 <conbody>
  <p><indexterm>origins<indexterm>overview</indexterm></indexterm>You can use different origins
      based on the execution mode of the pipeline. </p>
    <p>In standalone pipelines, you can use the following origins: <ul id="ul_mxz_jxm_jq">
        <li>Amazon S3 - Reads files from Amazon S3.</li>
        <li>Directory - Reads fully-written files from a directory. </li>
        <li>File Tail - Reads lines of data from an active file after reading related archived files
          in the directory. </li>
        <li>HTTP Client - Reads data from a streaming HTTP resource URL.</li>
        <li>JDBC Consumer - Reads database data through a JDBC connection.</li>
        <li>JMS Consumer - Reads messages from JMS. </li>
        <li>Kafka Consumer - Reads messages from Kafka.</li>
        <li>Kinesis Consumer - Reads data from Kinesis Streams.</li>
        <li>MapR Streams Consumer - Reads messages from MapR Streams.</li>
        <li>MongoDB - Reads documents from MongoDB.</li>
        <li>Omniture - Reads web usage reports from the Omniture reporting API.</li>
        <li>RabbitMQ Consumer - Reads messages from RabbitMQ.</li>
        <li>Redis Consumer - Reads messages from Redis.</li>
        <li>SDC RPC - Reads data from an SDC RPC destination in an SDC RPC pipeline.</li>
        <li>SDC RPC to Kafka - Reads data from an SDC RPC destination in an SDC RPC pipeline and
          writes it to Kafka.</li>
        <li>SFTP/FTP Client - Reads files from an SFTP or FTP server.</li>
        <li>UDP Source - Reads messages from one or more UDP ports. </li>
        <li>UDP to Kafka - Reads messages from one or more UDP ports and writes the data to
          Kafka.</li>
      </ul></p>
    <p>In cluster pipelines, you can use the following origins:<ul id="ul_unr_xhb_ws">
        <li>Hadoop FS - Reads data from the Hadoop Distributed File System (HDFS). </li>
        <li>Kafka Consumer - Reads messages from Kafka. Use the cluster version of the origin.</li>
      </ul></p>
    <p>To help create or test pipelines, you can use the following development origins:<ul
        id="ul_nr2_c1p_qv">
        <li>Dev Data Generator </li>
        <li>Dev Random Source</li>
        <li>Dev Raw Data Source </li>
      </ul></p>
    <p>For more information, see <xref href="../Pipeline_Design/DevStages.dita#concept_czx_ktn_ht"
      />.</p>
 </conbody>
</concept>
