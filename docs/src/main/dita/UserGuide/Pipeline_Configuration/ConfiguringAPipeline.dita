<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE task PUBLIC "-//OASIS//DTD DITA General Task//EN" "generalTask.dtd">
<task id="task_xlv_jdw_kq">
    <title>Configuring a Pipeline</title>
    <shortdesc>Configure a pipeline to define the stream of data. After you configure the pipeline,
        you can start the pipeline. </shortdesc>
    <taskbody>
        <context>
            <p><indexterm>pipelines<indexterm>configuring</indexterm></indexterm><indexterm>pipelines<indexterm>labels</indexterm></indexterm><indexterm>labels<indexterm>pipelines</indexterm></indexterm>A
                pipeline can include the following stages:<ul id="ul_eyy_b2w_kq">
                    <li>A single origin stage</li>
                    <li>Multiple processor stages</li>
                    <li>Multiple destination stages</li>
                </ul></p>
        </context>
        <steps id="steps_tsx_d2w_kq">
            <step conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/CreatePipeline1"
                conrefend="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/CreatePipeline2">
                <cmd/>
            </step>
            <step id="pipeProperties">
                <cmd>In the Properties panel, on the <wintitle>General</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_ryh_vfm_zs">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.0*"/>
                            <thead>
                                <row>
                                    <entry>Pipeline Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Description</entry>
                                    <entry>Optional description of the pipeline.</entry>
                                </row>
                                <row>
                                    <entry>Labels</entry>
                                    <entry>Optional labels to assign to the pipeline. Enter new or
                                        existing label names.<p>Use labels to group similar
                                            pipelines. For example, you might want to group
                                            pipelines by database schema or by the test or
                                            production environment.</p></entry>
                                </row>
                                <row>
                                    <entry>Execution Mode <xref href="../Cluster_Mode/ClusterPipelines.dita"
                                                ><image href="../Graphics/icon_moreInfo.png"
                                                scale="10" id="image_zfp_n5g_cs"/>
                                        </xref></entry>
                                    <entry>Execution mode of the pipeline:<ul id="ul_bn5_jsz_zr">
                                            <li>Standalone - A single <ph
                                                  conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                                /> process runs the pipeline.</li>
                                            <li>Cluster Batch - <ph
                                                  conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                                /> spawns additional workers as needed to process
                                                data in HDFS. Processes all available data and then
                                                stops the pipeline.</li>
                                            <li>Cluster Yarn Streaming - <ph
                                                  conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                                /> spawns additional workers as needed to process
                                                data. Use to stream data from a Kafka cluster that
                                                uses Spark Streaming on YARN.</li>
                                            <li>Cluster Mesos Streaming - <ph
                                                  conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                                /> spawns additional workers as needed to process
                                                data. Use to stream data from a Kafka cluster that
                                                uses Spark Streaming on Mesos.</li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Delivery Guarantee <xref
                                            href="../Pipeline_Design/DeliveryGuarantee.dita#concept_ffz_hhw_kq">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_rtg_yfm_zs"/></xref></entry>
                                    <entry>Determines how <ph
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> handles data after an unexpected event causes the
                                        pipeline to stop running:<ul id="ul_b4v_51m_sq">
                                            <li>At Least Once - Ensures all data is processed and
                                                written to the destination. Might result in
                                                duplicate rows.</li>
                                            <li>At Most Once - Ensures that data is not reprocessed
                                                to prevent writing duplicate data to the
                                                destination. Might result in missing rows.</li>
                                        </ul><p>Default is At Least Once.</p></entry>
                                </row>
                                <row>
                                    <entry>Retry Pipeline on Error <xref
                                            href="Retry.dita"
                                                ><image href="../Graphics/icon_moreInfo.png"
                                                scale="10" id="image_wms_qvz_2t"/>
                                        </xref></entry>
                                    <entry>Retries the pipeline upon error. </entry>
                                </row>
                                <row>
                                    <entry>Retry Attempts</entry>
                                    <entry>Number of retries attempted. Use -1 to retry
                                        indefinitely. <p>The wait time between retries starts at 15
                                            seconds and doubles until reaching five
                                        minutes.</p></entry>
                                </row>
                                <row>
                                    <entry>Max Pipeline Memory <xref
                                            href="PipelineMemory.dita"
                                                ><image href="../Graphics/icon_moreInfo.png"
                                                scale="10" id="image_ldk_s5g_cs"/>
                                        </xref></entry>
                                    <entry>Maximum amount of memory for the pipeline to use. You can
                                        enter a numeric value or edit the default expression to use
                                        a percentage of the <ph
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> Java heap size. <p>Default is 65% of the <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> Java heap
                                            size:<codeblock> ${jvm:maxMemoryMB() * 0.65}</codeblock></p></entry>
                                </row>
                                <row>
                                    <entry>On Memory Exceeded</entry>
                                    <entry>Action to take when the pipeline memory reaches the
                                        memory limit:<ul id="ul_vsf_3k4_1s">
                                            <li>Log - Logs a message in the pipeline history.</li>
                                            <li>Log and Alert - Logs a message and triggers an alert
                                                that displays in monitor mode and sends an alert
                                                email to any provided email addresses.</li>
                                        </ul><ul id="ul_mtf_3k4_1s">
                                            <li>Log, Alert and Stop Pipeline - Logs a message,
                                                triggers an alert that displays in monitor mode and
                                                sends an alert email to any provided email
                                                addresses. Stops the pipeline. This option is not
                                                supported for cluster mode pipelines at this time.
                                            </li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Notify on Pipeline State Changes</entry>
                                    <entry>Sends an email when the pipeline encounters the listed
                                        pipeline states. </entry>
                                </row>
                                <row>
                                    <entry>Email IDs</entry>
                                    <entry>Email addresses to receive notification about pipeline
                                        state changes. </entry>
                                </row>
                                <row>
                                    <entry>Rate Limit (records / sec)
                                        <xref
                                            href="PipelineRateLimit.dita"
                                            ><image href="../Graphics/icon_moreInfo.png"
                                                scale="10" id="image_lgk_s6g_cs"/>
                                        </xref></entry>
                                    <entry>Maximum number of records that the pipeline can read in a
                                        second. Use 0 or no value to set no rate limit.<p>Default is
                                            0.</p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>To define pipeline constants, on the <wintitle>Constants</wintitle> tab, click
                    the <uicontrol>Add</uicontrol> icon and define the name and the value for each
                    constant.</cmd>
            </step>
            <step>
                <cmd>Click the <wintitle>Error Records</wintitle> tab and configure the following
                    error handling option:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_ghf_x22_br">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.25*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.25*"/>
                            <thead>
                                <row>
                                    <entry>Error Records Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Error Records <xref
                                            href="../Pipeline_Design/ErrorHandling.dita#concept_pm4_txm_vq">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                            /></xref></entry>
                                    <entry>Determines how to handle records that cannot be processed
                                        as expected. Use one of the following options:<ul
                                            id="ul_ayr_y22_br">
                                            <li>Discard - Discards error records.</li>
                                            <li>Write to Another Pipeline - Writes error records to
                                                another pipeline. To use this option, you need an
                                                SDC RPC destination pipeline to process the error
                                                records. </li>
                                            <li>Write to Elasticsearch - Writes error records to the
                                                specified Elasticsearch cluster.</li>
                                            <li>Write to File - Writes error records to a file in
                                                the specified directory. </li>
                                            <li>Write to Kafka - Writes error records to the
                                                specified Kafka cluster.</li>
                                            <li>Write to MapR Streams - Writes error records to the
                                                specified MapR cluster.</li>
                                        </ul><p>For cluster mode, Write to File is not supported at
                                            this time. </p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>When writing error records to an SDC RPC pipeline, click the <wintitle>Error
                        Records - Write to Another Pipeline</wintitle> tab and configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_abt_n11_ft">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Write to Pipeline Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/row-RPCconnect">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/row-RPCID">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/row-SSLenabled">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/row-TrustStore">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/row-TSpass">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/row-verifyHost">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/row-RetriesBatch">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/row-ConTimeout">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/row-ReadTimeout">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/row-UseCompression">
                                    <entry/>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>When writing error records to Elasticsearch, click the <uicontrol>Error Records
                        - Write to Elasticsearch</uicontrol> tab and configure the following
                    properties:</cmd>
                <info
                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/ElasticProps-Info"/>
                <info>If you enabled Shield, configure the following Shield properties:</info>
                <info
                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/ElasticSHIELD-Info"
                />
            </step>
            <step>
                <cmd>When writing error records to file, click the <wintitle>Error Records - Write
                        to File</wintitle> tab and configure the following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_gy3_1y4_1r">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Write to File Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Directory</entry>
                                    <entry>Local directory for error record files. </entry>
                                </row>
                                <row>
                                    <entry>File Prefix</entry>
                                    <entry>Prefix used for error record files. Use to differentiate
                                        error record files from other files in the directory.<p>Uses
                                            the prefix sdc-${sdc:id()} by default. The prefix
                                            evaluates to sdc-&lt;<ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> ID>. This provides default differentiation in case
                                            several <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            />s write to the same directory. </p><p>The <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> ID is stored in the following file:
                                                <filepath>$SDC_DATA/sdc.id</filepath> file. For more information about environment variables, see <xref
                                                    href="../Install_Config/DCEnvironmentConfig.dita#concept_rng_qym_qr"/>.
                                        </p></entry>
                                </row>
                                <row>
                                    <entry>File Wait Time (secs)</entry>
                                    <entry>Number of seconds <ph
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> waits for error records. After that time, it creates a
                                        new error record file. <p>You can enter a number of seconds
                                            or use the default expression to enter the time in
                                            minutes. </p></entry>
                                </row>
                                <row>
                                    <entry>Max File Size (MB)</entry>
                                    <entry>Maximum size for error files. Exceeding this size creates
                                        a new error file. <p>Use 0 to avoid using this
                                        property.</p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>When writing error records to Kafka, click the <wintitle>Error Records - Write
                        to Kafka</wintitle> tab and configure the following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_lb5_svh_ms">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.25*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.25*"/>
                            <thead>
                                <row>
                                    <entry>Write to Kafka Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPBrokerURI">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPRuntimeTopic">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPTopicEx">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPTopicWList">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPTopic">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPPartStrategy">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPPartExpr">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPOneMessPBatch">
                                    <entry/>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/KPKConfigs">
                                    <entry/>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>When writing data to a MapR Streams cluster, click the <wintitle>Error Records
                        - Write to MapR Streams</wintitle> tab and configure the following
                    properties:</cmd>
                <info
                    conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/MapRStreams-Info"
                />
            </step>
            <step>
                <cmd>When using the cluster execution mode, click the <uicontrol>Cluster</uicontrol>
                    tab and configure the following properties:</cmd>
                <info>For Spark Streaming on Mesos, configure the following properties:<table
                        frame="all" rowsep="1" colsep="1" id="table_vb1_dsj_f5">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Mesos Cluster Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Dispatcher URL</entry>
                                    <entry>Master URL of the Mesos dispatcher. For example,
                                        mesos://dispatcher:7077.</entry>
                                </row>
                                <row>
                                    <entry>Hadoop/S3 Checkpoint Configuration Directory <xref
                                            href="../Cluster_Mode/CheckpointStorage.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                            /></xref></entry>
                                    <entry>Location of the HDFS configuration files that specify
                                        whether to write checkpoint metadata to HDFS or Amazon
                                            S3.<p>Use a directory or symlink within the <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> resources directory.</p><p>The directory should
                                            include the following files:<ul
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/HDFSfiles_HDFSdest"
                                                id="ul_qnc_jtt_bt">
                                                <li/>
                                            </ul></p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table></info>
                <info>For Spark Streaming or MapReduce on YARN, configure the following properties.
                        <table frame="all" rowsep="1" colsep="1" id="table_mkj_kdr_wr">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Yarn Cluster Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Worker Memory (MB)</entry>
                                    <entry>Maximum amount of memory allocated to each <ph
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> worker in the cluster.<p>Default is 1024 MB.</p></entry>
                                </row>
                                <row>
                                    <entry>Worker Java Options</entry>
                                    <entry>Additional Java properties for the pipeline. Separate
                                        properties with a space.<p>The following properties are set
                                            by default. </p><p>
                                            <ul id="ul_hf3_xqj_ws">
                                                <li>XX:PermSize is set to 128 MB and XX:MaxPermSize
                                                  is set to 256 MB. This defines the PermGen size
                                                  for the pipeline. </li>
                                                <li>Dlog4j.debug enables debug logging for
                                                  log4j.</li>
                                            </ul>
                                        </p><p>Changing the default properties is not
                                            recommended.</p><p>You can add any valid Java property.
                                        </p></entry>
                                </row>
                                <row>
                                    <entry>Launcher Env Configuration</entry>
                                    <entry>
                                        <p>Additional configuration properties for the cluster
                                            launcher. Click the <uicontrol>Add</uicontrol> icon and
                                            define the property name and value. </p>
                                    </entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table></info>
            </step>
            <step>
                <cmd>Use the <uicontrol>Stage Library</uicontrol> to add an origin stage. In the
                    Properties panel, configure the stage properties.</cmd>
                <info>For configuration details about origin stages, see <xref
                        href="../Origins/Origins_overview.dita#concept_hpr_twm_jq"/>.</info>
            </step>
            <step>
                <cmd>Use the <uicontrol>Stage Library</uicontrol> to add the next stage that you
                    want to use, connect the origin to the new stage, and configure the new
                    stage.</cmd>
                <info>For configuration details about processors, see <xref
                        href="../Processors/Processors_overview.dita#concept_hpr_twm_jq"/>.<p>For
                        configuration details about destinations, see <xref
                            href="../Destinations/Destinations_overview.dita#concept_hpr_twm_jq"
                        />.</p></info>
            </step>
            <step>
                <cmd>Add additional stages as necessary.</cmd>
            </step>
            <step>
                <cmd>At any point, you can use the <uicontrol>Preview</uicontrol> icon to preview
                    data to help .configure the pipeline. For more information, see <xref
                        href="../Data_Preview/DataPreview.dita#concept_jtn_s3m_lq"/>. </cmd>
            </step>
            <step>
                <cmd>Optionally, you can create metric or data alerts to track details about a
                    pipeline run and create threshold alerts. For more information, see <xref
                        href="../Alerts/RulesAlerts_title.dita#concept_pgk_brx_rr"/>.</cmd>
            </step>
            <step>
                <cmd>When the pipeline is complete, use the <uicontrol>Start</uicontrol> icon to run
                    the pipeline. </cmd>
            </step>
        </steps>
        <result>When <ph
                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
            starts the pipeline, the Monitor console displays real-time statistics for the pipeline.
            For more information about monitoring, see <xref
                href="../Pipeline_Monitoring/PipelineMonitoring.dita#concept_hsp_tnt_lq"/>.</result>
    </taskbody>
</task>
