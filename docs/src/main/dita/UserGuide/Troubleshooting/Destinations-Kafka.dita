<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_mth_cjm_js">
 <title>Kafka Producer</title>
 <conbody>
  <p>
   <dl>
    <dlentry>
     <dt>Can the Kafka Producer create topics?</dt>
     <dd>The Kafka Producer can create a topic when all of the following are true:<ul
       id="ul_j4q_25y_3r">
       <li>You configure the Kafka Producer to write to a topic name that does not exist.</li>
       <li>At least one of the Kafka brokers defined for the Kafka Producer has the
        auto.create.topics.enable property enabled.</li>
       <li>The broker with the enabled property is up and available when the Kafka Producer looks
        for the topic.</li>
      </ul></dd>
    </dlentry>
    <dlentry>
     <dt>How can I reset the offset for a Kafka Consumer?</dt>
     <dd>Since the offset for a Kafka Consumer is stored with the ZooKeeper for the Kafka cluster,
      you cannot reset the offset through the <ph
       conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>. For
      information about resetting an offset through Kafka, see the Apache Kafka documentation. </dd>
    </dlentry>
        <dlentry>
          <dt>A pipeline that writes to Kafka keeps failing and restarting in an endless cycle.</dt>
          <dd>This can happen when the pipeline tries to write message to Kafka 0.8 that is longer
            than the Kafka maximum message size. </dd>
          <dd>Workaround: Reconfigure Kafka brokers to allow larger messages or ensure that incoming
            records are within the configured limit. <draft-comment author="Loretta">This replaces a
              release notes known limitation for SDC-2438.</draft-comment></dd>
        </dlentry>
   </dl>
  </p>
 </conbody>
</concept>
