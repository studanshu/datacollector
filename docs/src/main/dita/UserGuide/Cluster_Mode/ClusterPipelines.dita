<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_hmh_kfn_1s">
 <title>Cluster Pipelines</title>
 <shortdesc>A <term>cluster pipeline</term> is a pipeline that runs in cluster execution mode. You
  can run a pipeline in standalone execution mode or cluster execution mode. </shortdesc>
 <conbody>
  <p><indexterm>execution mode<indexterm>standalone and cluster
    modes</indexterm></indexterm><indexterm>cluster
    mode<indexterm>description</indexterm></indexterm><indexterm>standalone
     mode<indexterm>description</indexterm></indexterm>In standalone mode, a single <ph
    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> process runs
   the pipeline. A pipeline runs in standalone mode by default. </p>
  <p>In cluster mode, the <ph
    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> uses a cluster
   manager and a cluster application to spawn additional workers as needed. Use cluster mode to read
   data from a Kafka cluster or HDFS.</p>
  <p>When would you choose standalone or cluster mode? Say you want to ingest logs from applications
   servers and perform a computationally expensive transformation. To do this, you might use a set
   of standalone pipelines to stream log data from each application server to Kafka. And then use a
   cluster pipeline to process the data from the Kafka cluster and perform the expensive
   transformation.</p>
  <p>Or, you might use cluster mode to move data from HDFS to another destination, such as
   Elasticsearch.</p>
  <p>The origin system determines how a <ph
    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> runs a cluster
   mode pipeline:<dl>
    <dlentry>
     <dt>Kafka cluster</dt>
     <dd>When processing data from a Kafka cluster, the <ph
       conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> processes
      data continuously until you stop the pipeline. This is known as cluster streaming mode. </dd>
     <dd>The <ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
      runs as an application within Spark Streaming, an open source cluster-computing application.
      Spark Streaming runs on either the Mesos or YARN cluster manager to process data from a Kafka
      cluster.</dd>
     <dd>The cluster manager and Spark Streaming spawn a <ph
       conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> worker for
      each partition in the Kafka cluster. So each partition has a <ph
       conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> worker to
      process data. </dd>
     <dd>Use the Kafka Consumer origin to process data from Kafka in cluster mode.</dd>
    </dlentry>
   </dl><dl>
    <dlentry>
     <dt>HDFS</dt>
     <dd>When processing data from HDFS, the <ph
       conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> processes
      all available data and then stops the pipeline. This is known as cluster batch mode.</dd>
     <dd>The <ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
      runs as an application on top of MapReduce, an open-source cluster-computing framework.
      MapReduce runs on the YARN cluster manager. </dd>
     <dd>YARN and MapReduce generate additional worker nodes as needed. MapReduce creates one map
      task for each HDFS block. </dd>
     <dd>Use the Hadoop FS origin to process data from HDFS in cluster mode.</dd>
    </dlentry>
   </dl></p>
  <p>You can use any processor or destination in cluster pipelines.</p>
 </conbody>
</concept>
