<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE task PUBLIC "-//OASIS//DTD DITA General Task//EN" "generalTask.dtd">
<task id="task_gmd_msw_yr">
    <title>Kafka Requirements</title>
    <taskbody>
        <context>
            <p><indexterm>cluster mode<indexterm>configuration for Kafka</indexterm></indexterm>Use
                the following requirements to configure a cluster mode pipeline to read from a Kafka
                cluster: <ol id="ol_e32_llw_yr">
                    <li>Verify the installation of Kafka, Spark Streaming, and either YARN or Mesos
                        as the cluster manager.</li>
                    <li>Install the <ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                        /> on a Spark and YARN or a Spark and Mesos gateway node.</li>
                    <li>If necessary, specify the location of the spark-submit script. <p>The <ph
                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                            /> assumes that the spark-submit script used to submit job requests to
                            Spark Streaming, is located in the following directory:
                            <codeblock>/usr/bin/spark-submit</codeblock></p>If the script is not in
                        this directory, use the following environment variables to define the
                        location of the script:<ul id="ul_xzc_bqj_f5">
                            <li>On YARN, use the SPARK_SUBMIT_YARN_COMMAND environment
                                variable.</li>
                            <li>On Mesos, use the SPARK_SUBMIT_MESOS_COMMAND environment
                                variable.</li>
                        </ul></li>
                    <li>In the pipeline properties, on the <wintitle>General</wintitle> tab, set the
                            <uicontrol>Execution Mode</uicontrol> property to
                            <uicontrol>Cluster</uicontrol>.</li>
                    <li>When running on Mesos, on the <uicontrol>Cluster</uicontrol> tab, enter the
                            <uicontrol>Dispatcher URL</uicontrol> and the <uicontrol>Hadoop/S3
                            Configuration Directory</uicontrol>. <p>The Hadoop/S3 Configuration
                            Directory is the location of the configuration files that determine
                            where the <ph
                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                            /> stores checkpoint metadata.</p></li>
                    <li>In the pipeline, use a Kafka Consumer origin for cluster mode. <p>If you
                            have more than one Kafka stage library installed, select the cluster
                            mode stage library on the General tab of the origin.</p></li>
                </ol><note>When you add a partition to the Kafka topic, restart the pipeline to
                    enable the <ph
                        conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                    /> to generate a new worker to read from the new partition. </note></p>
        </context>
    </taskbody>
    <related-links>
        <link href="../Pipeline_Configuration/ConfiguringAPipeline.dita#task_xlv_jdw_kq" type="topic"/>
        <link href="../Origins/KConsumer.dita#concept_msz_wnr_5q" type="topic"/>
    </related-links>
</task>
