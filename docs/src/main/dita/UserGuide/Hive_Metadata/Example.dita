<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
  
      http://www.apache.org/licenses/LICENSE-2.0
      
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_a1w_kkn_fw">
 <title>Case Study</title>
 <conbody>
  <p><indexterm>Hive Drift Solution<indexterm>case study</indexterm></indexterm>Let's say you have a
                <ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
            /> pipeline that writes log data to Kafka. The File Tail origin in the pipeline
            processes data from several different web services, tagging each record with a "tag"
            header attribute that identifies the service that generated the data. </p>
        <p>Now you want a new pipeline to pass the data to HDFS where it can be stored and reviewed,
            and you'd like the data written to tables based on the web service that generated the
            data. </p>
        <p>To do this, add and configure a Kafka Consumer to read the data into the pipeline, then
            connect it to a Hive Metadata processor. The processor assesses the record structure and
            generates a metadata record that describes any required Hive metadata changes. Using the
            tag header attribute and other user-defined expressions, a Hive Metadata processor can
            determine the database, table, and partition to use for the target directory and write
            that information along with the Avro schema to the record header, including file roll
            indicator when necessary.</p>
        <p>You connect the Hive Metadata processor metadata output stream to a Hive Metastore
            destination. The destination, upon receiving the metadata record from the Hive Metadata
            processor, creates or updates Hive tables as needed. </p>
        <p>You connect the Hive Metadata processor data output stream to a Hadoop FS destination and
            configure it to use the information in record headers. The destination then writes each
            record where it wants to go using the target directory and Avro schema in the record
            header, and rolling files when needed.</p>
        <p>Now let's take a closer look... </p>
 </conbody>
</concept>
