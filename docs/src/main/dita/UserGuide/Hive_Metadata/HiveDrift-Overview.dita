<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
  
      http://www.apache.org/licenses/LICENSE-2.0
      
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_phk_bdf_2w">
    <title>Hive Drift Solution: Ingesting Drifting Data into Hive</title>
    <conbody>
        <p><indexterm>Hive Drift Solution<indexterm>overview</indexterm></indexterm>The Hive Drift
            Solution detects drift in incoming data and updates corresponding Hive tables. The
            solution enables creating and updating Hive tables based on record requirements and
            writing data to HDFS based on record header attributes. You can use the full
            functionality of the solution or individual pieces, as needed. </p>
        <p>The Hive Drift Solution supports writing only Avro data to HDFS at this time.</p>
        <p>The solution incorporates the Hive Metadata processor, Hive Metastore destination, and
            the Hadoop FS destination as follows:</p>
        <p>
            <dl>
                <dlentry>
                    <dt>Drift detection</dt>
                    <dd>When processing records, the Hive Metadata processor detects columnar drift
                        and the need for new tables and partitions. It generates metadata records
                        that describe the necessary changes. </dd>
                    <dd>When the Hive Metastore destination receives a metadata record, it compares
                        the proposed changes with the latest Hive metadata, and creates and updates
                        Hive tables as needed.</dd>
                    <dd><ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/D-HM-CreatesAndNot_PH"
                        /></dd>
                </dlentry>
                <dlentry>
                    <dt>Record-based writes</dt>
                    <dd>The Hive Metadata processor also adds the following information to the
                        header of each record. The Hadoop FS destination writes data to HDFS based
                        on these details: <ul id="ul_sn1_bjg_2w">
                            <li>Target directory - Based on user-defined expressions, the Hive
                                Metadata processor assembles the path where each record should be
                                stored in HDFS. It writes the generated path to a
                                    <term>targetDirectory</term> attribute in each record
                                    header.<p>To write the record to the generated path, configure
                                    Hadoop FS to use the targetDirectory header attribute. </p></li>
                            <li>Avro schema - The processor writes the Avro schema to the
                                    <term>avroSchema</term> attribute in each record header. It
                                generates new Avro schemas when necessary based on the record
                                    structure.<p>To use the generated Avro schema, configure Hadoop
                                    FS to use the avroSchema header attribute.</p></li>
                            <li>Roll files - When a change in Avro schema occurs, the processor
                                generates a roll indicator - the <term>roll</term> header attribute.
                                    <p>To roll files based on schema changes, configure Hadoop FS
                                    use the roll header attribute.</p></li>
                        </ul></dd>
                </dlentry>
            </dl>
        </p>
        <p>For example, say you use this solution to write sales data to HDFS. A partial upgrade of
            the sales system adds several new fields to a subset of the incoming data. </p>
        <p>With the Hive Drift Solution, the Hive Metadata processor notes the new fields in a
            metadata record and passes it to the Hive Metastore destination. The Hive Metastore
            destination adds the new columns to the Hive target table. Hadoop FS then writes the
            data to the updated table. When writing data without the new fields to the updated
            table, Hadoop FS inserts null values for the missing fields. </p>
    </conbody>
</concept>
